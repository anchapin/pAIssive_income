#!/usr/bin/env python3
"""run_ui.py - Entry point for the Flask web application."""

from __future__ import annotations

import contextlib
import gzip
import logging
import logging.handlers
import os
import sys
import threading
import time
from datetime import datetime, timezone
from pathlib import Path
from queue import Queue
from typing import Any, Final, TypeVar, cast

import psutil
from flask.globals import current_app
from werkzeug.local import LocalProxy

from app_flask import create_app
from app_flask.middleware.logging_middleware import setup_request_logging
from common_utils.security import SecurityError, run_command_securely
from config import Config

# Type variable for generic typing
T = TypeVar("T")

# Type hint for Flask app logger
FlaskLogger = logging.Logger

logger: Final[LocalProxy[logging.Logger]] = cast(
    "LocalProxy[logging.Logger]", LocalProxy(lambda: current_app.logger)
)


class CompressedRotatingFileHandler(logging.handlers.TimedRotatingFileHandler):
    """Extended handler that compresses rotated logs."""

    def __init__(self, filename: str | Path, *args: Any, **kwargs: Any) -> None:
        """
        Initialize with compression delay.

        Args:
            filename: Path to the log file
            *args: Additional positional arguments for TimedRotatingFileHandler
            **kwargs: Additional keyword arguments for TimedRotatingFileHandler

        """
        self.compress_delay: float = kwargs.pop("compress_delay", 60.0)
        filename_str = str(filename) if isinstance(filename, Path) else filename
        super().__init__(filename_str, *args, **kwargs)

    def rotation_filename(self, default_name: str) -> str:
        """
        Generate compressed filename.

        Args:
            default_name: The default filename generated by TimedRotatingFileHandler

        Returns:
            str: The name of the compressed file with .gz extension

        """
        return f"{default_name}.gz"

    def rotate(self, source: str, dest: str) -> None:
        """
        Rotate and compress the log file.

        Args:
            source: Path to the source log file
            dest: Path to the destination compressed file

        """

        def delayed_compress() -> None:
            """Compress file after delay to avoid race conditions."""
            time.sleep(self.compress_delay)
            source_path = Path(source)
            with source_path.open("rb") as f_in, gzip.open(f"{dest}.gz", "wb") as f_out:
                f_out.writelines(f_in)
            with contextlib.suppress(OSError):
                source_path.unlink()

        threading.Thread(target=delayed_compress, daemon=True).start()


class ContextFilter(logging.Filter):
    """Add contextual information to log records."""

    def __init__(self) -> None:
        """Initialize system information."""
        super().__init__()
        self.process_start_time: float = time.time()
        self.process_id: int = os.getpid()
        self.process: psutil.Process = psutil.Process(self.process_id)

    def get_memory_usage(self) -> dict[str, float]:
        """
        Get current memory usage statistics.

        Returns:
            dict[str, float]: Memory usage statistics in MB

        """
        mem_info = self.process.memory_info()
        return {
            "rss": mem_info.rss / 1024 / 1024,  # Resident Set Size in MB
            "vms": mem_info.vms / 1024 / 1024,  # Virtual Memory Size in MB
        }

    def filter(self, record: logging.LogRecord) -> bool:
        """
        Add context information to log records.

        Args:
            record: The log record to augment

        Returns:
            bool: Always returns True to include the record

        """
        # Add process info
        record.process_id = self.process_id  # type: ignore
        record.process_uptime = time.time() - self.process_start_time  # type: ignore

        # Add memory info
        mem_usage = self.get_memory_usage()
        record.memory_rss = f"{mem_usage['rss']:.1f}MB"  # type: ignore
        record.memory_vms = f"{mem_usage['vms']:.1f}MB"  # type: ignore

        return True


def create_compressed_handler(
    log_dir: str | Path,
    filename: str,
    *,
    max_bytes: int = 10485760,  # 10MB
    backup_count: int = 5,
    compress_delay: float = 60.0,
) -> CompressedRotatingFileHandler:
    """
    Create a rotating file handler with compression.

    Args:
        log_dir: Directory to store log files
        filename: Name of the log file
        max_bytes: Maximum size of each log file
        backup_count: Number of backup files to keep
        compress_delay: Delay before compressing rotated files

    Returns:
        CompressedRotatingFileHandler: The configured handler

    """
    log_path = Path(log_dir) / filename
    return CompressedRotatingFileHandler(
        log_path,
        when="D",
        interval=1,
        backupCount=backup_count,
        compress_delay=compress_delay,
    )


def verify_log_files(*log_files: str | Path) -> None:
    """
    Verify log files are writable and create if needed.

    Args:
        *log_files: Paths to log files to verify

    """
    for log_file in log_files:
        path = Path(log_file)
        path.parent.mkdir(parents=True, exist_ok=True)

        if not path.exists():
            try:
                path.touch(mode=0o644)
            except OSError:
                logger.exception("Failed to create log file: %s", path)
                sys.exit(1)


def create_console_handler(level: str = Config.LOG_LEVEL) -> logging.StreamHandler:
    """
    Create a console log handler.

    Args:
        level: Minimum logging level

    Returns:
        logging.StreamHandler: The configured handler

    """
    handler = logging.StreamHandler()
    handler.setLevel(getattr(logging, level.upper()))
    handler.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
    return handler


def setup_file_formatter(use_json: bool = Config.LOG_FORMAT_JSON) -> logging.Formatter:
    """
    Create log formatter based on configuration.

    Args:
        use_json: Whether to use JSON formatting

    Returns:
        logging.Formatter: The configured formatter

    """
    if use_json:
        return logging.Formatter(
            '{"timestamp":"%(asctime)s","level":"%(levelname)s",'
            '"logger":"%(name)s","message":"%(message)s"}'
        )
    return logging.Formatter("%(asctime)s %(levelname)s [%(name)s] %(message)s")


def setup_all_handlers() -> tuple[list[logging.Handler], Queue[logging.LogRecord]]:
    """
    Set up all logging handlers.

    Returns:
        tuple[list[logging.Handler],
        Queue[logging.LogRecord]]: Tuple of handlers and log queue

    """
    handlers: list[logging.Handler] = []
    log_queue: Queue[logging.LogRecord] = Queue()

    # Console handler
    console = create_console_handler()
    handlers.append(console)

    # File handlers for different log types
    formatter = setup_file_formatter()

    log_handlers = [
        ("app.log", logging.INFO),
        ("error.log", logging.ERROR),
        ("debug.log", logging.DEBUG),
    ]

    for filename, level in log_handlers:
        handler = create_compressed_handler(Config.LOG_DIR, filename)
        handler.setLevel(level)
        handler.setFormatter(formatter)
        handlers.append(handler)

    # Queue handler for async logging
    queue_handler = logging.handlers.QueueHandler(log_queue)
    handlers.append(queue_handler)

    return handlers, log_queue


def setup_root_logger(level: str) -> logging.Logger:
    """
    Configure the root logger.

    Args:
        level: Minimum logging level

    Returns:
        logging.Logger: The configured root logger

    """
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, level.upper()))

    # Add context filter
    context_filter = ContextFilter()
    root_logger.addFilter(context_filter)

    return root_logger


def setup_logging() -> None:
    """Set up the entire logging system."""
    # Create log directory
    log_dir = Path(Config.LOG_DIR)
    log_dir.mkdir(parents=True, exist_ok=True)

    # Verify log files
    verify_log_files(log_dir / "app.log", log_dir / "error.log", log_dir / "debug.log")

    # Set up logger and handlers
    root_logger = setup_root_logger(Config.LOG_LEVEL)
    handlers, log_queue = setup_all_handlers()

    # Configure async logging
    stop_event = threading.Event()

    def handle_logs() -> None:
        """Process logs from queue."""
        while not stop_event.is_set() or not log_queue.empty():
            try:
                record = log_queue.get(timeout=0.1)
                for handler in handlers:
                    if not isinstance(handler, logging.handlers.QueueHandler):
                        handler.handle(record)
            except Exception:  # pylint: disable=broad-except
                if not stop_event.is_set():
                    root_logger.exception("Error processing log record")

    # Start logging thread
    logging_thread = threading.Thread(target=handle_logs, daemon=True)
    logging_thread.start()

    def cleanup() -> None:
        """Clean up logging on shutdown."""
        stop_event.set()
        logging_thread.join(timeout=5.0)
        for handler in handlers:
            handler.close()

    # Register cleanup
    import atexit

    atexit.register(cleanup)

    # Set up request logging
    setup_request_logging()

    root_logger.info("Logging system initialized")


def get_version() -> str:
    """
    Get the application version.

    Returns:
        str: The version string

    """
    try:
        with Path("VERSION").open() as f:
            return f.read().strip()
    except OSError:
        return "0.0.0"


def health_check() -> dict[str, str]:
    """
    Check application health.

    Returns:
        dict[str, str]: Health status information

    """
    status = "healthy"
    message = "Application is running normally"

    try:
        # Add additional health checks here
        pass
    except Exception as e:  # pylint: disable=broad-except
        status = "unhealthy"
        message = f"Health check failed: {e!s}"
        logger.exception("Health check failed")

    return {
        "status": status,
        "message": message,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": get_version(),
    }


def is_running_in_container() -> bool:
    """
    Check if the application is running in a container.

    This checks multiple container indicators:
    - Docker: /.dockerenv file
    - Kubernetes: /var/run/secrets/kubernetes.io
    - Generic: CONTAINER environment variable

    Returns:
        bool: True if running in a container, False otherwise

    """
    docker_env = Path("/.dockerenv").exists()
    k8s_env = Path("/var/run/secrets/kubernetes.io").exists()
    container_env = os.environ.get("CONTAINER", "").lower() == "true"

    return docker_env or k8s_env or container_env


def validate_host_binding(host: str, is_container: bool) -> None:
    """
    Validate the host binding configuration for security.

    Args:
        host: The host address to bind to
        is_container: Whether running in a container environment

    Raises:
        ValueError: If the host binding is potentially insecure

    """
    # Validate host format
    if not isinstance(host, str):
        msg = "Host must be a string"
        raise ValueError(msg)  # Only allow localhost or container bindings
    allowed_hosts = {"127.0.0.1", "localhost", "::1"}
    if is_container:
        allowed_hosts.add("0.0.0.0")  # nosec B104 - Explicitly allowed only in container environments with validation
        allowed_hosts.add("::")  # nosec B104 - Explicitly allowed only in container environments with validation

    if host not in allowed_hosts and not is_container:
        msg = (
            f"Invalid host binding '{host}'. Non-container environments must bind to "
            "localhost (127.0.0.1, ::1, or localhost) only."
        )
        raise ValueError(msg)  # Additional security check for debug mode
    if Config.DEBUG and host in {"0.0.0.0", "::", "0:0:0:0:0:0:0:0"}:  # nosec B104 - This is a security check explicitly validating these values
        msg = "Debug mode cannot be enabled when binding to all interfaces"
        raise ValueError(msg)  # Log security-relevant configuration with safe data
    safe_host = "[all-interfaces]" if host in {"0.0.0.0", "::"} else host  # nosec B104 - Just used for logging comparison, not binding
    logger.info(
        "Server binding configuration: host=%s, container=%s, debug=%s",
        safe_host,
        is_container,
        Config.DEBUG,
    )


def _run_command(cmd: list[str], **kwargs: Any) -> subprocess.CompletedProcess[str]:
    """Run a command securely."""
    try:
        return run_command_securely(cmd, **kwargs)
    except SecurityError as e:
        logger.exception("Security error running command: %s", e)
        raise
    except Exception as e:
        logger.exception("Error running command: %s", e)
        raise


def main() -> None:
    """Application entry point."""
    # Set up logging first
    setup_logging()

    # Check container environment
    is_container = is_running_in_container()

    # Create and configure Flask app
    app = create_app()

    # Only use 0.0.0.0 in containerized environments where needed and with appropriate warnings
    is_container = (
        os.environ.get("CONTAINER", "false").lower() in ("true", "1", "yes")
    )  # Default to localhost for security, only bind to all interfaces in container with warning
    if is_container:
        default_host = os.environ.get("BIND_HOST", "0.0.0.0")  # nosec B104 - Allow override via env var, only in container environments
        if hasattr(app, "logger"):
            app.logger.warning(
                "Binding to all network interfaces (%s). "
                "This is expected in container environments but may pose a security risk otherwise. "
                "Override with BIND_HOST environment variable if needed.",
                default_host,
            )
    else:
        default_host = os.environ.get(
            "BIND_HOST", "127.0.0.1"
        )  # Allow override but default to localhost

    host = os.environ.get("FLASK_HOST", default_host)
    port = int(os.environ.get("FLASK_PORT", "5000"))

    # Log startup information with extra context
    if hasattr(app, "logger"):
        app.logger.info(
            "Starting Flask application",
            extra={
                "host": host,
                "port": port,
                "container_mode": is_container,
            },
        )

    try:
        # Validate host binding security
        validate_host_binding(Config.HOST, is_container)

        # Start the application
        app.run(
            host=Config.HOST,
            port=Config.PORT,
            debug=Config.DEBUG,
            use_reloader=Config.USE_RELOADER,
        )
    except ValueError as e:
        logger.exception("Security configuration error: %s", str(e))
        sys.exit(1)
    except Exception:  # pylint: disable=broad-except
        logger.exception("Application startup failed")
        sys.exit(1)
